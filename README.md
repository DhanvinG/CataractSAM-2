# CataractSAM-2: Enhancing Transferability and Real-Time Ophthalmic Surgery Segmentation Through Automated Ground-Truth Generation

We introduce CataractSAMâ€‘2, a domain-adapted extension of [SAMâ€‘2](https://github.com/facebookresearch/sam2) optimized for high-precision segmentation in cataract and related ophthalmic surgeries. To preserve generalizable visual priors, we freeze the SAMâ€‘2 image encoder and fine-tune only the prompt encoder and mask decoder on the [Cataractâ€‘1K dataset](https://github.com/Negin-Ghamsarian/Cataract-1K). To address the time-consuming nature of manual frame-by-frame annotation, we develop a human-in-the-loop interactive annotation framework built on the `SAM2VideoPredictor`, significantly accelerating ground-truth generation.

<img width="1920" height="1440" alt="Image" src="https://github.com/user-attachments/assets/add66b99-f97c-4eab-9937-394ed09f0a43" />


## Overview

- **CataractSAMâ€‘2 Model**  
  A fine-tuned, domain-adapted variant of Metaâ€™s SAMâ€‘2, trained specifically for ophthalmic surgery segmentation. It achieves **90â€“95% mean IoU** and runs in real time at **15 FPS** across surgical videos.

- **Interactive Ground-Truth Annotation Framework**  
  A lightweight, point-guided annotation system leveraging the `SAM2VideoPredictor`. Users provide sparse point-based prompts, and the model propagates accurate masks through the video, cutting annotation time by **over 80%**.

- **Open-Source Toolkit**  
  This repo includes:
  - âœ… Pretrained weights (`.pth`)
  - âœ… Interactive inference widgets
  - âœ… Demo notebook


## Backup Repository:
An additional repository, independent of Google Colab, is available: https://github.com/mohaEs/CataractSAM-2

## Tutorial

https://github.com/user-attachments/assets/bbd12ae9-ce1b-4756-b7d3-bc71fd6f43b4

## Load from ðŸ¤— Hugging Face

We released our pretrain weight [here](https://huggingface.co/DhanvinG/Cataract-SAM2/tree/main)


## Installation
This project ships Meta's original SAM-2 repository as a git submodule under `sam2/`. Installing it in editable mode enables the exact CLI exposed by the upstream code. The environment requires Python 3.10+ and the packages listed in `requirements.txt`.  The weight download script fetches the public SAM-2 checkpoint from the `DhanvinG/Cataract-SAM2` repository on Hugging Face. It is stored in `checkpoints/` and is needed before using the library. CataractSAMâ€‘2 has been tested on **Pythonâ€¯3.12**, **SAMâ€‘2â€¯v1.0**, **Jupyter Notebookâ€¯7.4.4**, and **CUDAâ€¯12.2**. Follow these steps to get started:

1. **Clone the repository**  
   ```bash
   git clone --recurse-submodules https://github.com/DhanvinG/Cataract-SAM2.git
   cd Cataract-SAM2
   git submodule update --init --recursive

2. **Create & activate a new virtual environment**  
   ```bash
   python -m venv venv
   # macOS/Linux
   source venv/bin/activate
   # Windows
   venv\Scripts\activate

3. **Install SAMâ€‘2 core in editable mode**  
   ```bash
    pip install -e ./segment_anything_2

4. **Install CataractSAMâ€‘2 in editable mode**  
   ```bash
    pip install -e .

5. **Install Jupyter Notebook (for running the demo)**
   ```bash
    pip install notebook

5. **Download pretrained weights**
   ```bash
    python examples/download_checkpoints.py

> [!WARNING]
> Restart your Python session or runtime to ensure imports work.
> This is required for Hydra and editable installs to be registered correctly

------------------------------

## Quick start

Place your video frames as numbered JPEG files under the `data` directory
(e.g. `data/frames/000.jpg`, `001.jpg`, â€¦). Then build the predictor directly:

```python
from sam2.build_sam import build_sam2_video_predictor
pred = build_sam2_video_predictor(model_cfg, "checkpoints/Cataract-SAM2.pth", device="cuda")
setup(pred, "data")
Object(0, 1)  # start annotating object 1 on frame 0
```

Click positive/negative points to guide the model segmentation. 

You can visualize intermediate masks by pressing the `VISUALIZE` button in the notebook UI.


```python
from cataractsam2.ui_widget import Visualize
Visualize()
```

When satisfied with a single frame, propagate your objects through the
sequence:

```python
from cataractsam2.ui_widget import Propagate
Propagate(10)  #e.g. show every 10th frame for a quick check
```

Finally export masks for all frames and objects:

```python
from cataractsam2 import Masks
Masks("./masks")  # one PNG per frame/object
```

## Project structure

- `cataractsam2/` â€“ library code wrapping SAM-2 and the widget interface.
- `examples/download_checkpoints.py` â€“ helper script to obtain SAM-2
  weights from Hugging Face.
- `data/` â€“ place your frame sequences here (example frames included).
- `notebooks/` â€“ Contains an end-to-end demo notebook for using CataractSAM2 on video frames.

CataractSAM-2 builds upon Meta's Segment Anything Model 2.  The code is
licensed under the Apache License 2.0; see the `LICENSE` file for details.
